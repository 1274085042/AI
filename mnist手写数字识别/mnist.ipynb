{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnist手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist=input_data.read_data_sets(\"./materals\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显示数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "for idx in range(16):\n",
    "    plt.subplot(4,4,idx+1)\n",
    "    plt.axis('off')\n",
    "    plt.title('[{}]'.format(np.argmax(mnist.train.labels[idx])))\n",
    "    plt.imshow(mnist.train.images[idx].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义神经网络前向传播\n",
    "定义两个placeholder分别用于图像和lable数据，另外，定义一个bool类型的变量用于标识当前网络是否正在训练。\n",
    "为了让网络更高效的运行，多个数据会被组织成一个batch送入网络，两个placeholder的第一个维度就是batchsize，如果不确定batchsize，就置为None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(\"float\",[None,784],name='x')\n",
    "y=tf.placeholder(\"float\",[None,10],name='y')\n",
    "\n",
    "#因为输入的图片是展开后的一维向量，所以需要把一维向量还原成二维图片\n",
    "x_image=tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "#定义第一个卷积层\n",
    "with tf.name_scope('conv1'):\n",
    "    C1=tf.contrib.slim.conv2d(x_image,6,[5,5],padding='VALID',activation_fn=tf.nn.relu)\n",
    "\n",
    "#定义最大值池化\n",
    "with tf.name_scope('pool1'):\n",
    "    M2=tf.contrib.slim.max_pool2d(C1,[2,2],stride=[2,2],padding='VALID')\n",
    "\n",
    "#定义第二个卷积层\n",
    "with tf.name_scope('conv2'):\n",
    "    C3=tf.contrib.slim.conv2d(M2,16,[5,5],padding='VALID',activation_fn=tf.nn.relu)\n",
    "\n",
    "#定义最大值池化\n",
    "with tf.name_scope('pool2'):\n",
    "    M4=tf.contrib.slim.max_pool2d(C3,[2,3],stride=[2,2],padding='VALID')\n",
    "\n",
    "#定义两个全连接\n",
    "with tf.name_scope('fc1'):\n",
    "    M4_flat=tf.contrib.slim.flatten(M4)\n",
    "    FC5=tf.contrib.slim.fully_connected(M4_flat,120,activation_fn=tf.nn.relu)\n",
    "\n",
    "with tf.name_scope('fc2'):\n",
    "    FC6=tf.contrib.slim.fully_connected(FC5,84,activation_fn=tf.nn.relu)\n",
    "    \n",
    "#为防止过拟合，添加一个0.6的dropout，以40%的概率关闭全连接层中的神经元\n",
    "#需要注意的是，dropout仅在训练的时候使用，验证的时候，需要关闭dropout，所以验证时候的keep_prob是1.0。\n",
    "#dropout的输出最终送入一个隐层为10的全连接层，这个全连接层即为最后的分类器\n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prob=tf.placeholder(name='keep_prob',dtype=tf.float32)\n",
    "    FC6_drop=tf.nn.dropout(FC6,keep_prob)\n",
    "\n",
    "with tf.name_scope('fc3'):\n",
    "    output=tf.contrib.slim.fully_connected(FC6_drop,10,activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output,labels=y))\n",
    "\n",
    "l2_loss=tf.add_n([tf.nn.l2_loss(w) for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "\n",
    "for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    print(w.name)\n",
    "    tf.summary.histogram(w.name,w)  #对权重进行监控（histogram适用于矩阵）\n",
    "\n",
    "total_loss=cross_entropy_loss+7e-5*l2_loss\n",
    "tf.summary.scalar('cross_entropy_loss',cross_entropy_loss)\n",
    "tf.summary.scalar('l2_loss',l2_loss)\n",
    "tf.summary.scalar('total_loss',total_loss)\n",
    "\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.3).minimize(total_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评价指标\n",
    "注意，上面的网络，最后输出的是未经softmax的output，不是概率分布，要想看到概率分布，还要经过softmax。\n",
    "将输出的结果与正确结果进行对比，即可得到网络输出结果的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=tf.nn.softmax(output)\n",
    "correct=tf.equal(tf.argmax(y,1),tf.arg_max(pred,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "trainig_step = 1100\n",
    "\n",
    "#保存训练参数\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=tf.summary.merge_all()\n",
    "with tf.Session() as sess:\n",
    "    writer=tf.summary.FileWriter(\"logs/\",sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #定义验证集与测试集\n",
    "    validate_data={x:mnist.validation.images,y:mnist.validation.labels,keep_prob:1.0}\n",
    "    test_data={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0}\n",
    "    \n",
    "    for i in range(trainig_step):\n",
    "        xs,ys=mnist.train.next_batch(batch_size)\n",
    "        _,loss,rs=sess.run([optimizer,cross_entropy_loss,merged],feed_dict={x:xs,y:ys,keep_prob:0.6})\n",
    "        \n",
    "        writer.add_summary(rs,i)\n",
    "        \n",
    "        #每隔100次训练，打印一次损失值与验证准确率\n",
    "        if i>0 and i %100==0:\n",
    "            validate_accuray=sess.run(accuracy,feed_dict=validate_data)\n",
    "            print(\"After %d training steps, the loss is %g,the validation is %g\"^(i,loss,validate_accuray))\n",
    "            saver.save(sess,'./model.ckpt',global_step=i)\n",
    "            \n",
    "    print('Training is finished.')\n",
    "    acc=sess.run(accuracy,feed_dict=test_data)\n",
    "    print(\"The test accuracy is : \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
